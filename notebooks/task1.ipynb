{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846503bb",
   "metadata": {},
   "source": [
    "# Text Preprocessing and N-Gram Language Model\n",
    "\n",
    "## Tasks:\n",
    "1. **Data Selection and Preprocessing**: Apply text normalization (tokenization, remove stop words, remove punctuation/numbers, lemmatization)\n",
    "2. **N-Gram Analysis**: Calculate probability of sentences using Markov assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292ce83",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f74c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CRIZMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CRIZMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CRIZMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\CRIZMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\CRIZMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac762",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba4160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a95b7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      19579 non-null  object\n",
      " 1   text    19579 non-null  object\n",
      " 2   author  19579 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 459.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa04242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "EAP    7900\n",
      "MWS    6044\n",
      "HPL    5635\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['author'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91f61a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edcc7c",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Pipeline\n",
    "\n",
    "### 3.1 Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b33863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A comprehensive text preprocessing class that performs:\n",
    "    - Tokenization\n",
    "    - Lowercasing\n",
    "    - Removing punctuation and numbers\n",
    "    - Removing stop words\n",
    "    - Lemmatization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_punctuation_and_numbers(self, tokens):\n",
    "\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "        # Remove tokens that are purely numeric or contain numbers\n",
    "        tokens = [token for token in tokens if not any(char.isdigit() for char in token)]\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "\n",
    "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        tokens = self.remove_punctuation_and_numbers(tokens)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.lemmatize(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_to_text(self, text):\n",
    "        tokens = self.preprocess(text)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194bf5b0",
   "metadata": {},
   "source": [
    "### 3.2 Demonstrate Preprocessing on Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b918cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Example 1\n",
      "Original Text:\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "\n",
      "After Tokenization (48 tokens):\n",
      "['This', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'I', 'might']...\n",
      "\n",
      "After Removing Punctuation & Numbers (41 tokens):\n",
      "['This', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'I', 'might', 'make', 'its', 'circuit']...\n",
      "\n",
      "After Removing Stop Words (21 tokens):\n",
      "['process', 'however', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', 'might', 'make', 'circuit', 'return', 'point', 'whence', 'set', 'without', 'aware', 'fact', 'perfectly', 'uniform', 'seemed']...\n",
      "\n",
      "After Lemmatization (21 tokens):\n",
      "['process', 'however', 'afforded', 'mean', 'ascertaining', 'dimension', 'dungeon', 'might', 'make', 'circuit', 'return', 'point', 'whence', 'set', 'without', 'aware', 'fact', 'perfectly', 'uniform', 'seemed']...\n",
      "\n",
      "Final Preprocessed Text:\n",
      "process however afforded mean ascertaining dimension dungeon might make circuit return point whence set without aware fact perfectly uniform seemed wall\n",
      "\n",
      "-----\n",
      "Example 2\n",
      "Original Text:\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n",
      "\n",
      "After Tokenization (15 tokens):\n",
      "['It', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.']...\n",
      "\n",
      "After Removing Punctuation & Numbers (14 tokens):\n",
      "['It', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake']...\n",
      "\n",
      "After Removing Stop Words (6 tokens):\n",
      "['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake']...\n",
      "\n",
      "After Lemmatization (6 tokens):\n",
      "['never', 'occurred', 'fumbling', 'might', 'mere', 'mistake']...\n",
      "\n",
      "Final Preprocessed Text:\n",
      "never occurred fumbling might mere mistake\n",
      "\n",
      "-----\n",
      "Example 3\n",
      "Original Text:\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n",
      "\n",
      "After Tokenization (41 tokens):\n",
      "['In', 'his', 'left', 'hand', 'was', 'a', 'gold', 'snuff', 'box', ',', 'from', 'which', ',', 'as', 'he', 'capered', 'down', 'the', 'hill', ',']...\n",
      "\n",
      "After Removing Punctuation & Numbers (36 tokens):\n",
      "['In', 'his', 'left', 'hand', 'was', 'a', 'gold', 'snuff', 'box', 'from', 'which', 'as', 'he', 'capered', 'down', 'the', 'hill', 'cutting', 'all', 'manner']...\n",
      "\n",
      "After Removing Stop Words (19 tokens):\n",
      "['left', 'hand', 'gold', 'snuff', 'box', 'capered', 'hill', 'cutting', 'manner', 'fantastic', 'steps', 'took', 'snuff', 'incessantly', 'air', 'greatest', 'possible', 'self', 'satisfaction']...\n",
      "\n",
      "After Lemmatization (19 tokens):\n",
      "['left', 'hand', 'gold', 'snuff', 'box', 'capered', 'hill', 'cutting', 'manner', 'fantastic', 'step', 'took', 'snuff', 'incessantly', 'air', 'greatest', 'possible', 'self', 'satisfaction']...\n",
      "\n",
      "Final Preprocessed Text:\n",
      "left hand gold snuff box capered hill cutting manner fantastic step took snuff incessantly air greatest possible self satisfaction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_texts = df['text'].head(3).tolist()\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"-----\\nExample {i}\")\n",
    "    print(f\"Original Text:\\n{text}\\n\")\n",
    "    \n",
    "    # Show step-by-step preprocessing\n",
    "    tokens = preprocessor.tokenize(text)\n",
    "    print(f\"After Tokenization ({len(tokens)} tokens):\\n{tokens[:20]}...\\n\")\n",
    "    \n",
    "    tokens_no_punct = preprocessor.remove_punctuation_and_numbers(tokens)\n",
    "    print(f\"After Removing Punctuation & Numbers ({len(tokens_no_punct)} tokens):\\n{tokens_no_punct[:20]}...\\n\")\n",
    "    \n",
    "    tokens_no_stop = preprocessor.remove_stopwords(tokens_no_punct)\n",
    "    print(f\"After Removing Stop Words ({len(tokens_no_stop)} tokens):\\n{tokens_no_stop[:20]}...\\n\")\n",
    "    \n",
    "    tokens_lemmatized = preprocessor.lemmatize(tokens_no_stop)\n",
    "    print(f\"After Lemmatization ({len(tokens_lemmatized)} tokens):\\n{tokens_lemmatized[:20]}...\\n\")\n",
    "    \n",
    "    preprocessed_text = ' '.join(tokens_lemmatized)\n",
    "    print(f\"Final Preprocessed Text:\\n{preprocessed_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168985f",
   "metadata": {},
   "source": [
    "### 3.3 Apply Preprocessing to Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b03349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>process however afforded mean ascertaining dim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>left hand gold snuff box capered hill cutting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>lovely spring looked windsor terrace sixteen f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>finding nothing else even gold superintendent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  This process, however, afforded me no means of...   \n",
       "1  It never once occurred to me that the fumbling...   \n",
       "2  In his left hand was a gold snuff box, from wh...   \n",
       "3  How lovely is spring As we looked from Windsor...   \n",
       "4  Finding nothing else, not even gold, the Super...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  process however afforded mean ascertaining dim...  \n",
       "1         never occurred fumbling might mere mistake  \n",
       "2  left hand gold snuff box capered hill cutting ...  \n",
       "3  lovely spring looked windsor terrace sixteen f...  \n",
       "4  finding nothing else even gold superintendent ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(preprocessor.preprocess_to_text)\n",
    "df['preprocessed_tokens'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "df[['text', 'preprocessed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993de14b",
   "metadata": {},
   "source": [
    "## 4. N-Gram Language Model\n",
    "\n",
    "### 4.1 Build N-Gram Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8653dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    \"\"\"\n",
    "    N-Gram Language Model with Markov Assumption\n",
    "    Supports unigram, bigram, and trigram models with Laplace smoothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=2, smoothing=True):\n",
    "        \"\"\"\n",
    "        Initialize N-gram model\n",
    "        n: order of n-gram (1=unigram, 2=bigram, 3=trigram)\n",
    "        smoothing: whether to apply Laplace (add-1) smoothing\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, tokens_list):\n",
    "        \"\"\"\n",
    "        Train the model on a list of tokenized sentences\n",
    "        tokens_list: list of lists, where each inner list is a tokenized sentence\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.n}-gram model...\")\n",
    "        \n",
    "        for tokens in tokens_list:\n",
    "            # Add start and end tokens\n",
    "            padded_tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
    "            \n",
    "            # Update vocabulary\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            # Count n-grams and contexts\n",
    "            for i in range(len(padded_tokens) - self.n + 1):\n",
    "                ngram = tuple(padded_tokens[i:i + self.n])\n",
    "                context = tuple(padded_tokens[i:i + self.n - 1])\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "        \n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"Training complete!\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Number of unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        print(f\"Number of unique contexts: {len(self.context_counts)}\")\n",
    "        \n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"\n",
    "        Calculate probability of an n-gram using Markov assumption\n",
    "        P(w_n | w_1, ..., w_{n-1}) = Count(w_1, ..., w_n) / Count(w_1, ..., w_{n-1})\n",
    "        \"\"\"\n",
    "        if isinstance(ngram, str):\n",
    "            ngram = tuple(ngram.split())\n",
    "            \n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        if self.smoothing:\n",
    "            # Laplace smoothing: add 1 to numerator and vocab_size to denominator\n",
    "            numerator = self.ngram_counts[ngram] + 1\n",
    "            denominator = self.context_counts[context] + self.vocab_size\n",
    "        else:\n",
    "            numerator = self.ngram_counts[ngram]\n",
    "            denominator = self.context_counts[context]\n",
    "            \n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def sentence_probability(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculate the probability of an entire sentence\n",
    "        Using chain rule: P(sentence) = P(w1) * P(w2|w1) * P(w3|w1,w2) * ...\n",
    "        Returns both probability and log probability (to avoid underflow)\n",
    "        \"\"\"\n",
    "        # Add padding for start tokens\n",
    "        padded_tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']\n",
    "        \n",
    "        log_prob = 0.0\n",
    "        prob = 1.0\n",
    "        ngrams_used = []\n",
    "        \n",
    "        for i in range(len(padded_tokens) - self.n + 1):\n",
    "            ngram = tuple(padded_tokens[i:i + self.n])\n",
    "            ngram_prob = self.get_probability(ngram)\n",
    "            \n",
    "            if ngram_prob > 0:\n",
    "                log_prob += np.log(ngram_prob)\n",
    "                prob *= ngram_prob\n",
    "                ngrams_used.append((ngram, ngram_prob))\n",
    "            else:\n",
    "                # Handle zero probability\n",
    "                log_prob = float('-inf')\n",
    "                prob = 0.0\n",
    "                break\n",
    "                \n",
    "        return {\n",
    "            'probability': prob,\n",
    "            'log_probability': log_prob,\n",
    "            'ngrams': ngrams_used,\n",
    "            'perplexity': np.exp(-log_prob / len(ngrams_used)) if ngrams_used else float('inf')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc01432",
   "metadata": {},
   "source": [
    "### 4.2 Train Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36f0c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2-gram model...\n",
      "Training complete!\n",
      "Vocabulary size: 22338\n",
      "Number of unique 2-grams: 219727\n",
      "Number of unique contexts: 22339\n"
     ]
    }
   ],
   "source": [
    "bigram_model = NGramLanguageModel(n=2, smoothing=True)\n",
    "bigram_model.train(df['preprocessed_tokens'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595561d",
   "metadata": {},
   "source": [
    "### 4.3 Calculate Probabilities for 10 Sentences\n",
    "\n",
    "We'll select 10 diverse sentences from the dataset and calculate their probabilities using the Markov assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b79b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula: P(sentence) = P(w1|START) * P(w2|w1) * P(w3|w2) * ... * P(END|wn)\n",
      "Using Laplace (Add-1) Smoothing\n",
      "\n",
      "\n",
      "SENTENCE 1\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "The gigantic magnitude and the immediately available nature of the sum, dazzled and bewildered all who thought upon the topic.\n",
      "\n",
      "Preprocessed Tokens (11 tokens):\n",
      "['gigantic', 'magnitude', 'immediately', 'available', 'nature', 'sum', 'dazzled', 'bewildered', 'thought', 'upon', 'topic']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 7.58e-48\n",
      "Log Probability: -108.4982\n",
      "Perplexity: 8446.5581\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(gigantic | <START>) = 0.000095\n",
      "2. P(magnitude | gigantic) = 0.000089\n",
      "3. P(immediately | magnitude) = 0.000089\n",
      "4. P(available | immediately) = 0.000089\n",
      "5. P(nature | available) = 0.000090\n",
      "6. P(sum | nature) = 0.000088\n",
      "7. P(dazzled | sum) = 0.000089\n",
      "8. P(bewildered | dazzled) = 0.000090\n",
      "9. P(thought | bewildered) = 0.000089\n",
      "10. P(upon | thought) = 0.000175\n",
      "... and 2 more bigrams\n",
      "\n",
      "SENTENCE 2\n",
      "Author: MWS\n",
      "\n",
      "Original Text:\n",
      "Shall I disturb this calm by mingling in the world?\n",
      "\n",
      "Preprocessed Tokens (5 tokens):\n",
      "['shall', 'disturb', 'calm', 'mingling', 'world']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 1.91e-22\n",
      "Log Probability: -50.0086\n",
      "Perplexity: 4166.2538\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(shall | <START>) = 0.001813\n",
      "2. P(disturb | shall) = 0.000088\n",
      "3. P(calm | disturb) = 0.000089\n",
      "4. P(mingling | calm) = 0.000089\n",
      "5. P(world | mingling) = 0.000090\n",
      "6. P(<END> | world) = 0.001676\n",
      "\n",
      "SENTENCE 3\n",
      "Author: MWS\n",
      "\n",
      "Original Text:\n",
      "He had seen so many customs and witnessed so great a variety of moral creeds that he had been obliged to form an independant one for himself which had no relation to the peculiar notions of any one co...\n",
      "\n",
      "Preprocessed Tokens (33 tokens):\n",
      "['seen', 'many', 'custom', 'witnessed', 'great', 'variety', 'moral', 'creed', 'obliged', 'form', 'independant', 'one', 'relation', 'peculiar', 'notion', 'one', 'country', 'early', 'prejudice', 'course', 'influenced', 'judgement', 'formation', 'principle', 'raw', 'colledge', 'idea', 'strangely', 'mingled', 'deepest', 'deduction', 'penetrating', 'mind']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 2.17e-135\n",
      "Log Probability: -310.0748\n",
      "Perplexity: 9134.8017\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(seen | <START>) = 0.000549\n",
      "2. P(many | seen) = 0.000133\n",
      "3. P(custom | many) = 0.000088\n",
      "4. P(witnessed | custom) = 0.000089\n",
      "5. P(great | witnessed) = 0.000089\n",
      "6. P(variety | great) = 0.000219\n",
      "7. P(moral | variety) = 0.000089\n",
      "8. P(creed | moral) = 0.000089\n",
      "9. P(obliged | creed) = 0.000090\n",
      "10. P(form | obliged) = 0.000089\n",
      "... and 24 more bigrams\n",
      "\n",
      "SENTENCE 4\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "We went up stairs into the chamber where the body of Mademoiselle L'Espanaye had been found, and where both the deceased still lay.\n",
      "\n",
      "Preprocessed Tokens (10 tokens):\n",
      "['went', 'stair', 'chamber', 'body', 'mademoiselle', \"l'espanaye\", 'found', 'deceased', 'still', 'lay']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 1.95e-42\n",
      "Log Probability: -96.0412\n",
      "Perplexity: 6192.0095\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(went | <START>) = 0.000954\n",
      "2. P(stair | went) = 0.000089\n",
      "3. P(chamber | stair) = 0.000089\n",
      "4. P(body | chamber) = 0.000089\n",
      "5. P(mademoiselle | body) = 0.000177\n",
      "6. P(l'espanaye | mademoiselle) = 0.000268\n",
      "7. P(found | l'espanaye) = 0.000134\n",
      "8. P(deceased | found) = 0.000087\n",
      "9. P(still | deceased) = 0.000089\n",
      "10. P(lay | still) = 0.000219\n",
      "... and 1 more bigrams\n",
      "\n",
      "SENTENCE 5\n",
      "Author: HPL\n",
      "\n",
      "Original Text:\n",
      "Over those horrors the evil moon now hung very low, but the puffy worms of the sea need no moon to feed by.\n",
      "\n",
      "Preprocessed Tokens (11 tokens):\n",
      "['horror', 'evil', 'moon', 'hung', 'low', 'puffy', 'worm', 'sea', 'need', 'moon', 'feed']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 9.00e-49\n",
      "Log Probability: -110.6293\n",
      "Perplexity: 10088.1045\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(horror | <START>) = 0.000215\n",
      "2. P(evil | horror) = 0.000089\n",
      "3. P(moon | evil) = 0.000089\n",
      "4. P(hung | moon) = 0.000089\n",
      "5. P(low | hung) = 0.000134\n",
      "6. P(puffy | low) = 0.000089\n",
      "7. P(worm | puffy) = 0.000090\n",
      "8. P(sea | worm) = 0.000089\n",
      "9. P(need | sea) = 0.000088\n",
      "10. P(moon | need) = 0.000089\n",
      "... and 2 more bigrams\n",
      "\n",
      "SENTENCE 6\n",
      "Author: MWS\n",
      "\n",
      "Original Text:\n",
      "She listened to me as she had done to the narration of my adventures, and sometimes took an interest in this species of information; but she did not, as I did, look on it as an integral part of her be...\n",
      "\n",
      "Preprocessed Tokens (18 tokens):\n",
      "['listened', 'done', 'narration', 'adventure', 'sometimes', 'took', 'interest', 'specie', 'information', 'look', 'integral', 'part', 'obtained', 'could', 'put', 'universal', 'sense', 'touch']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 4.18e-76\n",
      "Log Probability: -173.5673\n",
      "Perplexity: 9275.3855\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(listened | <START>) = 0.000334\n",
      "2. P(done | listened) = 0.000089\n",
      "3. P(narration | done) = 0.000089\n",
      "4. P(adventure | narration) = 0.000089\n",
      "5. P(sometimes | adventure) = 0.000089\n",
      "6. P(took | sometimes) = 0.000089\n",
      "7. P(interest | took) = 0.000089\n",
      "8. P(specie | interest) = 0.000089\n",
      "9. P(information | specie) = 0.000089\n",
      "10. P(look | information) = 0.000089\n",
      "... and 9 more bigrams\n",
      "\n",
      "SENTENCE 7\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "His chief amusements were gunning and fishing, or sauntering along the beach and through the myrtles, in quest of shells or entomological specimens; his collection of the latter might have been envied...\n",
      "\n",
      "Preprocessed Tokens (17 tokens):\n",
      "['chief', 'amusement', 'gunning', 'fishing', 'sauntering', 'along', 'beach', 'myrtle', 'quest', 'shell', 'entomological', 'specimen', 'collection', 'latter', 'might', 'envied', 'swammerdamm']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 3.65e-73\n",
      "Log Probability: -166.7942\n",
      "Perplexity: 10576.0091\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(chief | <START>) = 0.000167\n",
      "2. P(amusement | chief) = 0.000089\n",
      "3. P(gunning | amusement) = 0.000089\n",
      "4. P(fishing | gunning) = 0.000090\n",
      "5. P(sauntering | fishing) = 0.000089\n",
      "6. P(along | sauntering) = 0.000090\n",
      "7. P(beach | along) = 0.000089\n",
      "8. P(myrtle | beach) = 0.000089\n",
      "9. P(quest | myrtle) = 0.000090\n",
      "10. P(shell | quest) = 0.000089\n",
      "... and 8 more bigrams\n",
      "\n",
      "SENTENCE 8\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "I have not the slightest fear for the result.\n",
      "\n",
      "Preprocessed Tokens (3 tokens):\n",
      "['slightest', 'fear', 'result']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 7.55e-16\n",
      "Log Probability: -34.8195\n",
      "Perplexity: 6032.2085\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(slightest | <START>) = 0.000119\n",
      "2. P(fear | slightest) = 0.000089\n",
      "3. P(result | fear) = 0.000088\n",
      "4. P(<END> | result) = 0.000802\n",
      "\n",
      "SENTENCE 9\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "I will content myself with saying, in addition, that my temperament is sanguine, rash, ardent, enthusiastic and that all my life I have been a devoted admirer of the women.\n",
      "\n",
      "Preprocessed Tokens (12 tokens):\n",
      "['content', 'saying', 'addition', 'temperament', 'sanguine', 'rash', 'ardent', 'enthusiastic', 'life', 'devoted', 'admirer', 'woman']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 1.82e-52\n",
      "Log Probability: -119.1349\n",
      "Perplexity: 9549.3250\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(content | <START>) = 0.000143\n",
      "2. P(saying | content) = 0.000089\n",
      "3. P(addition | saying) = 0.000089\n",
      "4. P(temperament | addition) = 0.000089\n",
      "5. P(sanguine | temperament) = 0.000089\n",
      "6. P(rash | sanguine) = 0.000090\n",
      "7. P(ardent | rash) = 0.000089\n",
      "8. P(enthusiastic | ardent) = 0.000089\n",
      "9. P(life | enthusiastic) = 0.000089\n",
      "10. P(devoted | life) = 0.000087\n",
      "... and 3 more bigrams\n",
      "\n",
      "SENTENCE 10\n",
      "Author: EAP\n",
      "\n",
      "Original Text:\n",
      "But my efforts were fruitless.\n",
      "\n",
      "Preprocessed Tokens (2 tokens):\n",
      "['effort', 'fruitless']\n",
      "\n",
      "--- PROBABILITY CALCULATION ---\n",
      "Sentence Probability: 1.52e-12\n",
      "Log Probability: -27.2095\n",
      "Perplexity: 8689.0745\n",
      "\n",
      "--- BIGRAM BREAKDOWN (First 10 bigrams) ---\n",
      "1. P(effort | <START>) = 0.000095\n",
      "2. P(fruitless | effort) = 0.000089\n",
      "3. P(<END> | fruitless) = 0.000179\n",
      "\n",
      "\n",
      " sentence_num author  original_length  token_count  probability  log_probability  perplexity\n",
      "            1    EAP              126           11     7.58e-48        -1.08e+02    8.45e+03\n",
      "            2    MWS               51            5     1.91e-22        -5.00e+01    4.17e+03\n",
      "            3    MWS              402           33    2.17e-135        -3.10e+02    9.13e+03\n",
      "            4    EAP              131           10     1.95e-42        -9.60e+01    6.19e+03\n",
      "            5    HPL              107           11     9.00e-49        -1.11e+02    1.01e+04\n",
      "            6    MWS              286           18     4.18e-76        -1.74e+02    9.28e+03\n",
      "            7    EAP              218           17     3.65e-73        -1.67e+02    1.06e+04\n",
      "            8    EAP               45            3     7.55e-16        -3.48e+01    6.03e+03\n",
      "            9    EAP              172           12     1.82e-52        -1.19e+02    9.55e+03\n",
      "           10    EAP               30            2     1.52e-12        -2.72e+01    8.69e+03\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(df), size=10, replace=False)\n",
    "sample_sentences = df.iloc[sample_indices]\n",
    "\n",
    "print(\"Formula: P(sentence) = P(w1|START) * P(w2|w1) * P(w3|w2) * ... * P(END|wn)\")\n",
    "print(\"Using Laplace (Add-1) Smoothing\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, (i, row) in enumerate(sample_sentences.iterrows(), 1):\n",
    "    original_text = row['text']\n",
    "    tokens = row['preprocessed_tokens']\n",
    "    author = row['author']\n",
    "    \n",
    "    # Calculate probability\n",
    "    result = bigram_model.sentence_probability(tokens)\n",
    "    \n",
    "    print(f\"\\nSENTENCE {idx}\")\n",
    "    print(f\"Author: {author}\")\n",
    "    print(f\"\\nOriginal Text:\")\n",
    "    print(f\"{original_text[:200]}{'...' if len(original_text) > 200 else ''}\")\n",
    "    print(f\"\\nPreprocessed Tokens ({len(tokens)} tokens):\")\n",
    "    print(f\"{tokens}\")\n",
    "    print(f\"\\n--- PROBABILITY CALCULATION ---\")\n",
    "    print(f\"Sentence Probability: {result['probability']:.2e}\")\n",
    "    print(f\"Log Probability: {result['log_probability']:.4f}\")\n",
    "    print(f\"Perplexity: {result['perplexity']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n--- BIGRAM BREAKDOWN (First 10 bigrams) ---\")\n",
    "    for j, (ngram, prob) in enumerate(result['ngrams'][:10], 1):\n",
    "        context = ngram[0]\n",
    "        word = ngram[1]\n",
    "        print(f\"{j}. P({word} | {context}) = {prob:.6f}\")\n",
    "    \n",
    "    if len(result['ngrams']) > 10:\n",
    "        print(f\"... and {len(result['ngrams']) - 10} more bigrams\")\n",
    "    \n",
    "    results.append({\n",
    "        'sentence_num': idx,\n",
    "        'author': author,\n",
    "        'original_length': len(original_text),\n",
    "        'token_count': len(tokens),\n",
    "        'probability': result['probability'],\n",
    "        'log_probability': result['log_probability'],\n",
    "        'perplexity': result['perplexity']\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cfe67",
   "metadata": {},
   "source": [
    "### 4.4 Train and Compare Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3-gram model...\n",
      "Training complete!\n",
      "Vocabulary size: 22338\n",
      "Number of unique 3-grams: 261283\n",
      "Number of unique contexts: 213534\n",
      "\n",
      "\n",
      "\n",
      " Sentence  Bigram_LogProb  Trigram_LogProb  Bigram_Perplexity  Trigram_Perplexity\n",
      "        1       -1.08e+02        -1.11e+02           8.45e+03            1.07e+04\n",
      "        2       -5.00e+01        -5.29e+01           4.17e+03            6.77e+03\n",
      "        3       -3.10e+02        -3.15e+02           9.13e+03            1.06e+04\n",
      "        4       -9.60e+01        -9.94e+01           6.19e+03            8.37e+03\n",
      "        5       -1.11e+02        -1.11e+02           1.01e+04            1.04e+04\n",
      "        6       -1.74e+02        -1.75e+02           9.28e+03            1.02e+04\n",
      "        7       -1.67e+02        -1.67e+02           1.06e+04            1.08e+04\n",
      "        8       -3.48e+01        -3.70e+01           6.03e+03            1.04e+04\n",
      "        9       -1.19e+02        -1.21e+02           9.55e+03            1.08e+04\n",
      "       10       -2.72e+01        -2.79e+01           8.69e+03            1.09e+04\n"
     ]
    }
   ],
   "source": [
    "trigram_model = NGramLanguageModel(n=3, smoothing=True)\n",
    "trigram_model.train(df['preprocessed_tokens'].tolist())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "trigram_results = []\n",
    "\n",
    "for idx, (i, row) in enumerate(sample_sentences.iterrows(), 1):\n",
    "    tokens = row['preprocessed_tokens']\n",
    "    result = trigram_model.sentence_probability(tokens)\n",
    "    \n",
    "    trigram_results.append({\n",
    "        'sentence_num': idx,\n",
    "        'probability': result['probability'],\n",
    "        'log_probability': result['log_probability'],\n",
    "        'perplexity': result['perplexity']\n",
    "    })\n",
    "\n",
    "# Compare bigram vs trigram\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Sentence': [r['sentence_num'] for r in results],\n",
    "    'Bigram_LogProb': [r['log_probability'] for r in results],\n",
    "    'Trigram_LogProb': [r['log_probability'] for r in trigram_results],\n",
    "    'Bigram_Perplexity': [r['perplexity'] for r in results],\n",
    "    'Trigram_Perplexity': [r['perplexity'] for r in trigram_results]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
